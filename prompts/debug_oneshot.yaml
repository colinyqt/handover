---
name: "Debug One-Shot Clause Extraction"
description: "Debug version with extensive logging to identify where the pipeline is failing"
version: "1.0-debug"

inputs:
  - name: "analysis_file"
    type: "file"
    description: "Tender analysis output file with extracted clauses and specifications"

processing_steps:
  # Step 0: Debug document content
  - name: "debug_document_content"
    description: "Show what content we're working with"
    type: python
    code: |
      content = context['inputs']['analysis_file']['content']
      print(f"[DEBUG] Document length: {len(content)} characters")
      print(f"[DEBUG] First 500 characters: {content[:500]}")
      print(f"[DEBUG] Document contains 'accuracy': {'accuracy' in content.lower()}")
      print(f"[DEBUG] Document contains 'voltage': {'voltage' in content.lower()}")
      print(f"[DEBUG] Document contains 'current': {'current' in content.lower()}")
      result = {'content_preview': content[:1000], 'length': len(content)}

  # Step 1: Simple chunk (no complex regex)
  - name: "simple_chunk"
    description: "Simple chunking for debugging"
    type: python
    code: |
      content = context['inputs']['analysis_file']['content']
      # Simple fixed-size chunks for debugging
      chunk_size = 2000
      chunks = [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)]
      chunks = [c.strip() for c in chunks if c.strip()]
      print(f"[DEBUG] Created {len(chunks)} chunks")
      for i, chunk in enumerate(chunks[:3]):  # Show first 3 chunks
          print(f"[DEBUG] Chunk {i+1} length: {len(chunk)}")
          print(f"[DEBUG] Chunk {i+1} preview: {chunk[:200]}...")
      result = {'chunks': chunks}

  # Step 2: Test one chunk with LLM
  - name: "test_single_extraction"
    description: "Test extraction on just the first chunk"
    type: llm
    dependencies: [simple_chunk]
    input: |
      You are testing requirements extraction. 
      
      From the following text, extract ANY technical requirements, measurements, or specifications you can find.
      
      Return ONLY valid JSON in this exact format:
      {
        "clause": "name or section found",
        "features": ["requirement 1", "requirement 2"],
        "text": "original text snippet"
      }
      
      If you find nothing technical, return:
      {
        "clause": "No technical content",
        "features": [],
        "text": "No technical requirements found"
      }
      
      Text to analyze:
      {{dep_simple_chunk['chunks'][0]}}
    output_key: test_extraction
    timeout: 60

  # Step 3: Debug the LLM output
  - name: "debug_llm_output"
    description: "Debug what the LLM actually returned"
    type: python
    dependencies: [test_single_extraction]
    code: |
      import json
      llm_output = context.get('dep_test_single_extraction', 'NO OUTPUT')
      print(f"[DEBUG] LLM output type: {type(llm_output)}")
      print(f"[DEBUG] LLM output content: {llm_output}")
      
      # Try to parse if it's a dict with raw_response
      if isinstance(llm_output, dict):
          if 'raw_response' in llm_output:
              print(f"[DEBUG] Raw response found: {llm_output['raw_response']}")
              try:
                  parsed = json.loads(llm_output['raw_response'])
                  print(f"[DEBUG] Parsed successfully: {parsed}")
                  result = {'parsed_output': parsed, 'success': True}
              except Exception as e:
                  print(f"[DEBUG] Parse error: {e}")
                  result = {'parse_error': str(e), 'success': False}
          else:
              print(f"[DEBUG] Dict without raw_response: {llm_output}")
              result = {'direct_dict': llm_output, 'success': True}
      else:
          print(f"[DEBUG] Non-dict output: {llm_output}")
          result = {'non_dict_output': str(llm_output), 'success': False}

  # Step 4: Test feature condensation if extraction worked
  - name: "test_feature_condensation"
    description: "Test condensing features if we got any"
    type: llm
    dependencies: [debug_llm_output]
    input: |
      Test condensing these sample features:
      - "True RMS current per phase & neutral ±0.5%"
      - "Operating voltage 400V/230V, 50Hz"
      - "Built-in memory ≥ 36 months"
      
      Return ONLY valid JSON mapping original to condensed:
      {
        "True RMS current per phase & neutral ±0.5%": "RMS current ±0.5%",
        "Operating voltage 400V/230V, 50Hz": "voltage 400V/230V 50Hz",
        "Built-in memory ≥ 36 months": "memory ≥36 months"
      }
    output_key: test_condensation
    timeout: 60

  # Step 5: Debug condensation output
  - name: "debug_condensation_output"
    description: "Debug the condensation LLM output"
    type: python
    dependencies: [test_feature_condensation]
    code: |
      import json
      condensation_output = context.get('dep_test_feature_condensation', 'NO OUTPUT')
      print(f"[DEBUG] Condensation output type: {type(condensation_output)}")
      print(f"[DEBUG] Condensation output: {condensation_output}")
      
      if isinstance(condensation_output, dict) and 'raw_response' in condensation_output:
          raw_text = condensation_output['raw_response']
          print(f"[DEBUG] Raw condensation response: {raw_text}")
          try:
              # Clean JSON markers
              if '```json' in raw_text:
                  raw_text = raw_text.split('```json\n', 1)[1].split('```', 1)[0]
              elif '```' in raw_text:
                  raw_text = raw_text.split('```', 1)[1].split('```', 1)[0]
              parsed = json.loads(raw_text.strip())
              print(f"[DEBUG] Condensation parsed successfully: {parsed}")
              result = {'condensed_features': parsed, 'success': True}
          except Exception as e:
              print(f"[DEBUG] Condensation parse error: {e}")
              result = {'condensation_error': str(e), 'success': False}
      else:
          result = {'condensation_raw': str(condensation_output), 'success': False}

  # Step 6: Generate summary report
  - name: "generate_debug_report"
    description: "Generate a comprehensive debug report"
    type: python
    dependencies: [debug_document_content, simple_chunk, debug_llm_output, debug_condensation_output]
    code: |
      report_lines = []
      report_lines.append("=== PIPELINE DEBUG REPORT ===\n")
      
      # Document analysis
      doc_info = context['dep_debug_document_content']
      report_lines.append(f"Document Length: {doc_info['length']} characters")
      report_lines.append(f"Content Preview: {doc_info['content_preview'][:200]}...\n")
      
      # Chunking results
      chunk_info = context['dep_simple_chunk']
      report_lines.append(f"Chunks Created: {len(chunk_info['chunks'])}")
      if chunk_info['chunks']:
          report_lines.append(f"First Chunk Length: {len(chunk_info['chunks'][0])}")
          report_lines.append(f"First Chunk Preview: {chunk_info['chunks'][0][:300]}...\n")
      
      # LLM extraction results
      llm_debug = context.get('dep_debug_llm_output', {})
      report_lines.append("LLM Extraction Results:")
      if llm_debug.get('success'):
          report_lines.append("✅ LLM extraction successful")
          if 'parsed_output' in llm_debug:
              parsed = llm_debug['parsed_output']
              report_lines.append(f"  - Clause: {parsed.get('clause', 'N/A')}")
              report_lines.append(f"  - Features found: {len(parsed.get('features', []))}")
              for feat in parsed.get('features', [])[:3]:  # Show first 3
                  report_lines.append(f"    * {feat}")
          else:
              report_lines.append(f"  - Direct output: {llm_debug.get('direct_dict', 'N/A')}")
      else:
          report_lines.append("❌ LLM extraction failed")
          report_lines.append(f"  - Error: {llm_debug.get('parse_error', 'Unknown error')}")
          report_lines.append(f"  - Raw output: {llm_debug.get('non_dict_output', 'N/A')}")
      
      report_lines.append("")
      
      # Condensation results
      condensation_debug = context.get('dep_debug_condensation_output', {})
      report_lines.append("Feature Condensation Results:")
      if condensation_debug.get('success'):
          report_lines.append("✅ Feature condensation successful")
          condensed = condensation_debug.get('condensed_features', {})
          report_lines.append(f"  - Condensed {len(condensed)} features")
          for orig, condensed_val in list(condensed.items())[:3]:  # Show first 3
              report_lines.append(f"    * '{orig}' → '{condensed_val}'")
      else:
          report_lines.append("❌ Feature condensation failed")
          report_lines.append(f"  - Error: {condensation_debug.get('condensation_error', 'Unknown error')}")
      
      report_lines.append("\n=== RECOMMENDATIONS ===")
      
      if not llm_debug.get('success'):
          report_lines.append("1. Check LLM model configuration and API connectivity")
          report_lines.append("2. Verify document contains technical content")
          report_lines.append("3. Test with a simpler document first")
      elif not condensation_debug.get('success'):
          report_lines.append("1. Check feature condensation prompt format")
          report_lines.append("2. Verify JSON parsing logic")
          report_lines.append("3. Test condensation with manual features")
      else:
          report_lines.append("1. Basic LLM steps are working - check full pipeline integration")
          report_lines.append("2. Test with foreach loops on multiple chunks")
          report_lines.append("3. Verify FAISS database connectivity")
      
      result = {'report': '\n'.join(report_lines)}

outputs:
  - type: "text"
    filename: "debug_pipeline_report_{{ timestamp }}.txt"
    content: |
      {{ step_results.generate_debug_report.report }}
